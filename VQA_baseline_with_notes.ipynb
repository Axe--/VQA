{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A28RYzUI5mHo"
   },
   "source": [
    "\n",
    "# **Server and workspace setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9u-IceGIhCmE"
   },
   "outputs": [],
   "source": [
    "# ''' <-- here\n",
    "# Run this cell ONLY ONCE when connecting\n",
    "# Once connected, uncomment the first line!\n",
    "googleDriveAlreadySetUp = False\n",
    "sshServerAlreadySetUp = False\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaKljkPKgQPF"
   },
   "source": [
    "## **Step 1. Mount the google drive to */content***\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "1. The *`/content`* directory is the only place where Google Colab allows file-writing. However, it is NOT persistent between each connection! (All change you make here will be erased once disconnected!!)\n",
    "\n",
    "2. And, Google Colab stays in /content would not `cd` to anyother folders (both inside and outside /content)!\n",
    "\n",
    "   ```\n",
    "        ! pwd >>> /content\n",
    "        ! cd ../ | pwd >>> /content\n",
    "        ! cd ~ | pwd >>> /content\n",
    "        ! cd / | pwd >>> /content\n",
    "        ! cd /content/gdrive | pwd >>> /content\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "3. So, all newly downloaded/created files & folders **(through \"`!-commands`\" on this notebook interface without an abosolute path)** automatically go to *`/content`*!! \n",
    "\n",
    "\n",
    "4. However, if you are **accessing the server using ssh** (see Step 2), you can make changes in 2 places: \n",
    "        a) /content\n",
    "        b) your google drive folder (wherever it is mounted to)\n",
    "        \n",
    "5. The conclusion: \n",
    "\n",
    "    **a) only make changes in /content/gdrive**\n",
    "\n",
    "    **b) always use absolute path in Colab notebook**\n",
    "    \n",
    "6. For shared drives: everything is the same as gdrive/My\\ Drive. \n",
    "    - expecting some file lock features to prevent simultaneous editing \n",
    "    - but didn't experient on that yet (#learning_through_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19412,
     "status": "ok",
     "timestamp": 1571459207982,
     "user": {
      "displayName": "Nuan Wen",
      "photoUrl": "",
      "userId": "11630876576360007648"
     },
     "user_tz": 420
    },
    "id": "qPVx49_BLeK6",
    "outputId": "08e1a4c0-c75a-44a8-be8a-6604cef7571e"
   },
   "outputs": [],
   "source": [
    "if not googleDriveAlreadySetUp:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    googleDriveAlreadySetUp = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTVSnQCIgr1E"
   },
   "source": [
    "## **Step 2. Server setup**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notes: \n",
    "\n",
    "1. Once setup, go to your choice of terminal and do ***`ssh root@0.tcp.ngrok.io -p [port#]`*** and enter the generated random password, you're in!\n",
    "\n",
    "2. Sometimes you will encounter the following error message:\n",
    "    ```\n",
    "    Traceback (most recent call last):\n",
    "    File \"\", line 1, in \n",
    "    IndexError: list index out of range\n",
    "    ```\n",
    "   This may (or may not) be because of the following line:\n",
    "   ```\n",
    "   ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "   ```\n",
    "   \n",
    "    *#TODO rewrite this line with python, not linux command.*\n",
    "   \n",
    "3. Yet, the connection has been setup. To see the portal number, just run the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55547,
     "status": "ok",
     "timestamp": 1571460642440,
     "user": {
      "displayName": "Nuan Wen",
      "photoUrl": "",
      "userId": "11630876576360007648"
     },
     "user_tz": 420
    },
    "id": "jB0n5oPrg3vk",
    "outputId": "128d35f7-3ade-483a-ce3b-18f2cf948c5c"
   },
   "outputs": [],
   "source": [
    "if not sshServerAlreadySetUp:\n",
    "    #Generate root password\n",
    "    import random, string\n",
    "    global password\n",
    "    password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(20))\n",
    "\n",
    "    #Download ngrok\n",
    "    ! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "    ! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
    "    #Setup sshd\n",
    "    ! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
    "    #Set root password\n",
    "    ! echo root:$password | chpasswd\n",
    "    ! mkdir -p /var/run/sshd\n",
    "    ! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
    "    ! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
    "    ! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc\n",
    "    ! echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n",
    "\n",
    "    #Run sshd\n",
    "    get_ipython().system_raw('/usr/sbin/sshd -D &')\n",
    "\n",
    "    #Ask token\n",
    "    print(\"Copy authtoken from https://dashboard.ngrok.com/auth\")\n",
    "    import getpass\n",
    "    authtoken = getpass.getpass()\n",
    "\n",
    "    #Create tunnel\n",
    "    get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n",
    "    #Print root password\n",
    "    print(\"Root password: {}\".format(password))\n",
    "    #Get public address\n",
    "    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "    \n",
    "    # finished setup\n",
    "    print(\"ssh server already setup, do ssh root@0.tcp.ngrok.io -p [port# see below] \")\n",
    "    sshServerAlreadySetUp = True # could the bug be here? no more line after a indented command?\n",
    "    \n",
    "else:\n",
    "    print(\"ssh server already setup, do ssh root@0.tcp.ngrok.io -p [port# see below] \")   \n",
    "    print(\"password\", password)\n",
    "    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "        \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5kXFL4KfylA"
   },
   "source": [
    "## Optional git/github/ssh-keygen/ssh-add setup\n",
    "\n",
    "\n",
    "- For once, run `ssh-keygen` and save id_ras in /content/gdrive/My\\ Drive/.ssh/id_rsa and save the public key in github setting\n",
    "\n",
    "- Every time connected to server, \n",
    "    1. run `eval $(ssh-agent)`\n",
    "    2. run `ssh-add /content/gdrive/My\\ Drive/.ssh/id_rsa`\n",
    "    3. set up local username & email (everytime if in Shared Drive (multiple users), just once if in personal drive):\n",
    "```\n",
    "     git config user.name \"your-user-name\"\n",
    "     git config user.email \"your-email-addr\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gs-ktZp3lhMO"
   },
   "outputs": [],
   "source": [
    "! eval $(ssh-agent)\n",
    "! cat /content/gdrive/My\\ Drive/.ssh/id_rsa.pub\n",
    "# Run the following command via ssh server to avoid \"Could not open a connection to your authentication agent.\"\n",
    "# ! ssh-add /content/gdrive/My\\ Drive/.ssh/id_rsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMZXExnjjVG-"
   },
   "source": [
    "**Step 3. Workplace Setup and Raw File Preparation**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Directory path: /content/gdrive/Shared\\ drives/VQA\n",
    "\n",
    "We (Shikhar and Nuan) decided to only work on VQA v2 with only open-ended answers.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notes on file system:\n",
    "1. `wget -P []` specifies the directory to download to, if not existing will be created.\n",
    "\n",
    "2. Have to use absolute path since we cannot change current directory (run `pwd` will always return `/content`) in G-Colab notebook.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notes on data collection:\n",
    "\n",
    "\n",
    "1. Every image has several free-form natural-language questions with 10 concise open-ended answers each.\n",
    "\n",
    "2. The annotations \"we\" release are the result of the following post-processing steps on the raw crowdsourced data:\n",
    "    - Spelling correction (using Bing Speller) of question and answer strings\n",
    "    - Question normalization (first char uppercase, last char ‘?’)\n",
    "    - Answer normalization (all chars lowercase, no period except as decimal point, number words —> digits, strip articles (a, an the))\n",
    "    - Adding apostrophe if a contraction is missing it (e.g., convert \"dont\" to \"don't\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Notes on folder structure:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "root@addd4560b196:/content/gdrive/Shared drives/VQA# tree -a -C -L 4 ./\n",
    "./\n",
    "├── data\n",
    "│   ├── datahelper (https://github.com/GT-Vision-Lab/VQA)\n",
    "│   │   └── ...\n",
    "│   ├── raw zip\n",
    "│   |   ├── Annotations_Train_abstract_v002.zip\n",
    "│   |   └── Questions_Train_abstract_v002.zip\n",
    "│   └── abstract scene\n",
    "│   │   └── train                                                               \n",
    "│   │       ├── abstract_v002_train2015_annotations.json\n",
    "│   │       ├── MultipleChoice_abstract_v002_train2015_questions.json\n",
    "│   │       ├── OpenEnded_abstract_v002_train2015_questions.json\n",
    "│   │       └── images\n",
    "│   │           ├── abstract_v002_train2015_000000000000.png\n",
    "│   │           ├── abstract_v002_train2015_000000000001.png\n",
    "│   │           ├── ...\n",
    "│   │           ├── abstract_v002_train2015_000000019998.png\n",
    "│   │           └── abstract_v002_train2015_000000019999.png\n",
    "└── VQA_baseline_with_notes.ipynb\n",
    "\n",
    "39 directories, 43 files\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvZtMQrvLrXr"
   },
   "outputs": [],
   "source": [
    "# Abstract Scene (same as v1.0 release)\n",
    "print(\"=============================================================================================\")\n",
    "print(\"\\nCollecting raw training data for abstract scenes...\\n\")\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "'''Training Annotations'''\n",
    "\n",
    "# check if Annotations_Train_abstract_v002.zip is downloaded, if not, download it\n",
    "! test -f /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip/Annotations_Train_abstract_v002.zip \\\n",
    "    && echo \"Annotations_Train_abstract_v002.zip already here, skip download\" \\\n",
    "    || { echo \"Annotations_Train_abstract_v002.zip does not exist, start downloading...\"; \\\n",
    "         wget https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/vqa/Annotations_Train_abstract_v002.zip \\\n",
    "                -P /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip;}\n",
    "/\n",
    "\n",
    "# check if Annotations_Train_abstract_v002.zip is unzipped, if not, unzip it\n",
    "! test -f /content/gdrive/Shared\\ drives/VQA/data/abstract\\ scene/train/abstract_v002_train2015_annotations.json \\\n",
    "    && echo \"abstract_v002_train2015_annotations.json already here, skip unzip\" \\\n",
    "    || { echo \"abstract_v002_train2015_annotations.json does not exist, start unzipping...\"; \\\n",
    "         unzip /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip/Annotations_Train_abstract_v002.zip \\\n",
    "               -d /content/gdrive/Shared\\ drives/VQA/data/abstract\\ scene/train;}\n",
    "/\n",
    "\n",
    "''' Training Questions '''\n",
    "# check if Questions_Train_abstract_v002.zip is downloaded, if not, download it\n",
    "! test -f /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip/Questions_Train_abstract_v002.zip \\\n",
    "    && echo \"Questions_Train_abstract_v002.zip already here, skip download\" \\\n",
    "    || { echo \"Questions_Train_abstract_v002.zip does not exist, start downloading...\"; \\\n",
    "         wget https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/vqa/Questions_Train_abstract_v002.zip \\\n",
    "                -P /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip;}\n",
    "/\n",
    "\n",
    "# check if Questions_Train_abstract_v002.zip is unzipped, if not, unzip it\n",
    "! test -f /content/gdrive/Shared\\ drives/VQA/data/abstract\\ scene/train/OpenEnded_abstract_v002_train2015_questions.json \\\n",
    "    && echo \"OpenEnded(MultipleChoice)_abstract_v002_train2015_questions.json already here, skip unzip\" \\\n",
    "    || { echo \"OpenEnded(MultipleChoice)_abstract_v002_train2015_questions.json does not exist, start unzipping...\"; \\\n",
    "         unzip /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip/Questions_Train_abstract_v002.zip \\\n",
    "               -d /content/gdrive/Shared\\ drives/VQA/data/abstract\\ scene/train;}\n",
    "/\n",
    "\n",
    "''' Training Images '''\n",
    "# check if scene_img_abstract_v002_train2015.zip is downloaded, if not, download it\n",
    "! test -f /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip/scene_img_abstract_v002_train2015.zip \\\n",
    "    && echo \"scene_img_abstract_v002_train2015.zip already here, skip download\" \\\n",
    "    || { echo \"scene_img_abstract_v002_train2015.zip does not exist, start downloading...\"; \\\n",
    "         wget https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/scene_img/scene_img_abstract_v002_train2015.zip \\\n",
    "                -P /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip;}\n",
    "/\n",
    "\n",
    "# check if Questions_Train_abstract_v002.zip is unzipped, if not, unzip it\n",
    "! test -f /content/gdrive/Shared\\ drives/VQA/data/abstract\\ scene/train/images/abstract_v002_train2015_000000000001.png \\\n",
    "    && echo \"abstract scene/train/images already here, skip unzip\" \\\n",
    "    || { echo \"abstract scene/train/images does not exist, start unzipping...\"; \\\n",
    "         unzip /content/gdrive/Shared\\ drives/VQA/data/raw\\ zip/scene_img_abstract_v002_train2015.zip \\\n",
    "               -d /content/gdrive/Shared\\ drives/VQA/data/abstract\\ scene/train/images;}\n",
    "/\n",
    "print()\n",
    "\n",
    "print(\"raw data for abstract training collected.\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_lxxSf_QA34"
   },
   "source": [
    "Similarly, MSCOCO_2014 (\"balanced real images\") datasets are collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EcFSsdRFwTz"
   },
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Goal: \n",
    "\n",
    "To write a script that will convert the VQA dataset in the following format (in a .txt file):\n",
    "\n",
    "> img_path \\t question \\t answer\n",
    "\n",
    "> (no space between each string)\n",
    "\n",
    "\n",
    "\n",
    "Note that img_path should preferrably contain only the file name (e.g. img_1.jpg), instead of the entire path (/home/axe/.../img_1.jpg).\n",
    "Also, you may write the question & answer in comma-separated style.   (e.g. where,is,he,?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qW5ezciWF3dS"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n",
    "\n",
    "# For Python 3.5+ use:\n",
    "\n",
    "# import importlib.util\n",
    "# spec = importlib.util.spec_from_file_location(\"module.name\", \"/path/to/file.py\")\n",
    "# foo = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(foo)\n",
    "# foo.MyClass()\n",
    "\n",
    "# MODULE_PATH = \"/content/gdrive/Shared drives/VQA/data/datahelper/PythonHelperTools/vqaTools/__init__.py\"\n",
    "# MODULE_NAME = \"vqaTools\"\n",
    "# import importlib\n",
    "# import sys\n",
    "# spec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)\n",
    "# vqaTools = importlib.util.module_from_spec(spec)\n",
    "# sys.modules[spec.name] = vqaTools \n",
    "# spec.loader.exec_module(vqaTools)\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"/content/gdrive/Shared drives/VQA/data/datahelper/PythonHelperTools/vqaTools\")\n",
    "# from vqaTools.vqa import VQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbSQAri1C6Y1"
   },
   "outputs": [],
   "source": [
    "'''/content/gdrive/Shared drives/VQA/data/datahelper/PythonHelperTools/vqaTools was written in Python 2.7 need to modify it to Python 3.7'''\n",
    "# change VQA from python2.7 to python3.7 Done\n",
    "\n",
    "# __author__ = 'aagrawal'\n",
    "# https://github.com/GT-Vision-Lab/VQA/blob/master/PythonHelperTools/vqaTools/vqa.py\n",
    "\n",
    "# Interface for accessing the VQA dataset.\n",
    "\n",
    "# This code is based on the code written by Tsung-Yi Lin for MSCOCO Python API available at the following link: \n",
    "# (https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/coco.py).\n",
    "\n",
    "# The following functions are defined:\n",
    "#  VQA        - VQA class that loads VQA annotation file and prepares data structures.\n",
    "#  getQuesIds - Get question ids that satisfy given filter conditions.\n",
    "#  getImgIds  - Get image ids that satisfy given filter conditions.\n",
    "#  loadQA     - Load questions and answers with the specified question ids.\n",
    "#  showQA     - Display the specified questions and answers.\n",
    "#  loadRes    - Load result file and create result object.\n",
    "\n",
    "# Help on each function can be accessed by: \"help(COCO.function)\"\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "class VQA:\n",
    "    def __init__(self, annotation_file=None, question_file=None):\n",
    "        \"\"\"\n",
    "           Constructor of VQA helper class for reading and visualizing questions and answers.\n",
    "        :param annotation_file (str): location of VQA annotation file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # load dataset\n",
    "        self.dataset = {}\n",
    "        self.questions = {}\n",
    "        self.qa = {}\n",
    "        self.qqa = {}\n",
    "        self.imgToQA = {}\n",
    "        if not annotation_file == None and not question_file == None:\n",
    "            print('loading VQA annotations and questions into memory...')\n",
    "            time_t = datetime.datetime.utcnow()\n",
    "            dataset = json.load(open(annotation_file, 'r'))\n",
    "            questions = json.load(open(question_file, 'r'))\n",
    "            print(datetime.datetime.utcnow() - time_t)\n",
    "            self.dataset = dataset\n",
    "            self.questions = questions\n",
    "            self.createIndex()\n",
    "\n",
    "    def createIndex(self):\n",
    "        # create index\n",
    "        print('creating index...')\n",
    "        imgToQA = {ann['image_id']: [] for ann in self.dataset['annotations']}\n",
    "        qa =  {ann['question_id']:       [] for ann in self.dataset['annotations']}\n",
    "        qqa = {ann['question_id']:       [] for ann in self.dataset['annotations']}\n",
    "        for ann in self.dataset['annotations']:\n",
    "            imgToQA[ann['image_id']] += [ann]\n",
    "            qa[ann['question_id']] = ann\n",
    "        for ques in self.questions['questions']:\n",
    "              qqa[ques['question_id']] = ques\n",
    "        print('index created!')\n",
    "\n",
    "         # create class members\n",
    "        self.qa = qa\n",
    "        self.qqa = qqa\n",
    "        self.imgToQA = imgToQA\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Print information about the VQA annotation file.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for key, value in self.dataset['info'].items():\n",
    "            print('%s: %s'%(key, value))\n",
    "\n",
    "    def getQuesIds(self, imgIds=[], quesTypes=[], ansTypes=[]):\n",
    "        \"\"\"\n",
    "        Get question ids that satisfy given filter conditions. default skips that filter\n",
    "        :param     imgIds    (int array)   : get question ids for given imgs\n",
    "                quesTypes (str array)   : get question ids for given question types\n",
    "                ansTypes  (str array)   : get question ids for given answer types\n",
    "        :return:    ids   (int array)   : integer array of question ids\n",
    "        \"\"\"\n",
    "        imgIds       = imgIds    if type(imgIds)    == list else [imgIds]\n",
    "        quesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\n",
    "        ansTypes  = ansTypes  if type(ansTypes)  == list else [ansTypes]\n",
    "\n",
    "        if len(imgIds) == len(quesTypes) == len(ansTypes) == 0:\n",
    "            anns = self.dataset['annotations']\n",
    "        else:\n",
    "            if not len(imgIds) == 0:\n",
    "                anns = sum([self.imgToQA[imgId] for imgId in imgIds if imgId in self.imgToQA],[])\n",
    "            else:\n",
    "                 anns = self.dataset['annotations']\n",
    "            anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n",
    "            anns = anns if len(ansTypes)  == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\n",
    "        ids = [ann['question_id'] for ann in anns]\n",
    "        return ids\n",
    "\n",
    "    def getImgIds(self, quesIds=[], quesTypes=[], ansTypes=[]):\n",
    "        \"\"\"\n",
    "        Get image ids that satisfy given filter conditions. default skips that filter\n",
    "        :param quesIds   (int array)   : get image ids for given question ids\n",
    "               quesTypes (str array)   : get image ids for given question types\n",
    "               ansTypes  (str array)   : get image ids for given answer types\n",
    "        :return: ids     (int array)   : integer array of image ids\n",
    "        \"\"\"\n",
    "        quesIds   = quesIds   if type(quesIds)   == list else [quesIds]\n",
    "        quesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\n",
    "        ansTypes  = ansTypes  if type(ansTypes)  == list else [ansTypes]\n",
    "\n",
    "        if len(quesIds) == len(quesTypes) == len(ansTypes) == 0:\n",
    "            anns = self.dataset['annotations']\n",
    "        else:\n",
    "            if not len(quesIds) == 0:\n",
    "                anns = sum([self.qa[quesId] for quesId in quesIds if quesId in self.qa],[])\n",
    "            else:\n",
    "                anns = self.dataset['annotations']\n",
    "            anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n",
    "            anns = anns if len(ansTypes)  == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\n",
    "        ids = [ann['image_id'] for ann in anns]\n",
    "        return ids\n",
    "\n",
    "    def loadQA(self, ids=[]):\n",
    "        \"\"\"\n",
    "        Load questions and answers with the specified question ids.\n",
    "        :param ids (int array)       : integer ids specifying question ids\n",
    "        :return: qa (object array)   : loaded qa objects\n",
    "        \"\"\"\n",
    "        if type(ids) == list:\n",
    "            return [self.qa[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.qa[ids]]\n",
    "\n",
    "    def showQA(self, anns):\n",
    "        \"\"\"\n",
    "        Display the specified annotations.\n",
    "        :param anns (array of object): annotations to display\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if len(anns) == 0:\n",
    "            return 0\n",
    "        for ann in anns:\n",
    "            quesId = ann['question_id']\n",
    "            print(\"Question: %s\" %(self.qqa[quesId]['question']))\n",
    "            for ans in ann['answers']:\n",
    "                print(\"Answer %d: %s\" %(ans['answer_id'], ans['answer']))\n",
    "        \n",
    "    def loadRes(self, resFile, quesFile):\n",
    "        \"\"\"\n",
    "        Load result file and return a result object.\n",
    "        :param   resFile (str)     : file name of result file\n",
    "        :return: res (obj)         : result api object\n",
    "        \"\"\"\n",
    "        res = VQA()\n",
    "        res.questions = json.load(open(quesFile))\n",
    "        res.dataset['info'] = copy.deepcopy(self.questions['info'])\n",
    "        res.dataset['task_type'] = copy.deepcopy(self.questions['task_type'])\n",
    "        res.dataset['data_type'] = copy.deepcopy(self.questions['data_type'])\n",
    "        res.dataset['data_subtype'] = copy.deepcopy(self.questions['data_subtype'])\n",
    "        res.dataset['license'] = copy.deepcopy(self.questions['license'])\n",
    "\n",
    "        print('Loading and preparing results...')\n",
    "        time_t = datetime.datetime.utcnow()\n",
    "        anns   = json.load(open(resFile))\n",
    "        assert type(anns) == list, 'results is not an array of objects'\n",
    "        annsQuesIds = [ann['question_id'] for ann in anns]\n",
    "        assert set(annsQuesIds) == set(self.getQuesIds()), \\\n",
    "        'Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.'\n",
    "        for ann in anns:\n",
    "            quesId                  = ann['question_id']\n",
    "            if res.dataset['task_type'] == 'Multiple Choice':\n",
    "                assert ann['answer'] in self.qqa[quesId]['multiple_choices'], 'predicted answer is not one of the multiple choices'\n",
    "            qaAnn                = self.qa[quesId]\n",
    "            ann['image_id']      = qaAnn['image_id'] \n",
    "            ann['question_type'] = qaAnn['question_type']\n",
    "            ann['answer_type']   = qaAnn['answer_type']\n",
    "        print('DONE (t=%0.2fs)'%((datetime.datetime.utcnow() - time_t).total_seconds()))\n",
    "\n",
    "        res.dataset['annotations'] = anns\n",
    "        res.createIndex()\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPdtKMxSasC7"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Prepare data for balanced real images QA aka COCO')\n",
    "\n",
    "parser.add_argument('--inp_dir', type=str, help='path to ../balanced real/ directory', required=True)\n",
    "parser.add_argument('--label_loc', type=str, help='location to store first 1000 most frequent answers', required=True)\n",
    "parser.add_argument('--dictionary_loc', type=str, help='location to store index2word and word2index dictionaries', required=True)\n",
    "parser.add_argument('--out_dir', type=str, help='output directory', required=True)\n",
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4067,
     "status": "ok",
     "timestamp": 1571461370697,
     "user": {
      "displayName": "Nuan Wen",
      "photoUrl": "",
      "userId": "11630876576360007648"
     },
     "user_tz": 420
    },
    "id": "nGDj57CrQiFH",
    "outputId": "28b8224b-cfff-4213-c76e-dfb67a01fdbb"
   },
   "outputs": [],
   "source": [
    "annFile = \"/content/gdrive/Shared drives/VQA/data/abstract scene/train/abstract_v002_train2015_annotations.json\"\n",
    "quesFile = \"/content/gdrive/Shared drives/VQA/data/abstract scene/train/OpenEnded_abstract_v002_train2015_questions.json\"\n",
    "\n",
    "# initialize VQA api for QA annotations\n",
    "\n",
    "vqa=VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1571461377080,
     "user": {
      "displayName": "Nuan Wen",
      "photoUrl": "",
      "userId": "11630876576360007648"
     },
     "user_tz": 420
    },
    "id": "BgIV5t_iC-Mz",
    "outputId": "c2e336f8-d569-4206-ffeb-8dfb34330494"
   },
   "outputs": [],
   "source": [
    "vqa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1571461384782,
     "user": {
      "displayName": "Nuan Wen",
      "photoUrl": "",
      "userId": "11630876576360007648"
     },
     "user_tz": 420
    },
    "id": "C5McFJR51GAE",
    "outputId": "c10d72d5-8879-43e8-d5d3-a68478f744e1"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "print(\"type(vqa.dataset):\\n\\t\", type(vqa.dataset), \"\\n\")\n",
    "print(\"vqa.dataset.keys():\\n\\t\", vqa.dataset.keys(), \"\\n\")\n",
    "print(\"type(vqa.dataset['annotations']):\\n\\t\", type(vqa.dataset['annotations']), \"\\n\")\n",
    "print(\"type(vqa.dataset['annotations'][0]):\\n\\t\", type(vqa.dataset['annotations'][0]), \"\\n\")\n",
    "print(\"vqa.dataset['annotations'][0].keys():\\n\\t\", vqa.dataset['annotations'][0].keys(), \"\\n\")\n",
    "print(\"one example:\", vqa.dataset['annotations'][-1])\n",
    "# most confident &/ most frequent ==> just use the multiple_choice_answer\n",
    "\n",
    "print(\"\\nnumber of answers:\", len(vqa.dataset['annotations']))\n",
    "\n",
    "imgQuesAnsTupList = []\n",
    "for i in range(len(vqa.dataset['annotations'])):\n",
    "    imgID = vqa.dataset['annotations'][i]['image_id']\n",
    "    quesID = vqa.dataset['annotations'][i]['question_id']\n",
    "    ansStr = vqa.dataset['annotations'][i]['multiple_choice_answer']\n",
    "    imgQuesAnsTupList.append((imgID, vqa.qqa[quesID]['question'], ansStr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60356,
     "status": "error",
     "timestamp": 1571461450405,
     "user": {
      "displayName": "Nuan Wen",
      "photoUrl": "",
      "userId": "11630876576360007648"
     },
     "user_tz": 420
    },
    "id": "5_Y5vlrIAS-u",
    "outputId": "1ff51902-d687-4fab-8ac4-ff3fdcaa2ca9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import random\n",
    "\n",
    "index = random.randint(0, 19999)\n",
    "\n",
    "print(imgQuesAnsTupList[index])\n",
    "imgIdStr = str(imgQuesAnsTupList[index][0]) if len(str(imgQuesAnsTupList[index][0])) == 5 else \"0\"+str(imgQuesAnsTupList[index][0])\n",
    "imgStr = \"abstract_v002_train2015_0000000\" + imgIdStr + \".png\"\n",
    "path = \"/content/gdrive/Shared drives/VQA/data/abstract scene/train/images/\"\n",
    "display(Image(filename=path+imgStr))\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
